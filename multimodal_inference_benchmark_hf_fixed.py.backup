#!/usr/bin/env python3

import argparse
import json
import time
import os
import requests
from pathlib import Path
from typing import List, Dict, Any, Optional
import statistics
import warnings

# Handle optional imports gracefully
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    warnings.warn("PIL not available. Image processing will be limited.")

try:
    import torch
    from transformers import (
        AutoProcessor, AutoModelForVision2Seq, 
        LlavaNextProcessor, LlavaNextForConditionalGeneration,
        LlavaProcessor, LlavaForConditionalGeneration,
        Qwen2VLForConditionalGeneration, AutoTokenizer
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    warnings.warn("Transformers not available. Please install transformers package.")

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False


def get_system_info():
    """Get system information for benchmarking context."""
    info = {
        "cpu_count": "unknown",
        "memory_total_gb": "unknown",
        "gpus": []
    }
    
    if PSUTIL_AVAILABLE:
        info.update({
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3)
        })
    
    # Simple GPU check without GPUtil
    if torch.cuda.is_available():
        try:
            gpu_count = torch.cuda.device_count()
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                info["gpus"].append({
                    "name": props.name,
                    "memory_total": props.total_memory // (1024**2),  # MB
                    "compute_capability": f"{props.major}.{props.minor}"
                })
        except Exception as e:
            warnings.warn(f"Error getting GPU info: {e}")
    
    return info


def download_sample_images_from_hf(output_dir: str, max_images: int = 20, dataset_name: str = None) -> List[Path]:
    """Download sample images from HuggingFace datasets."""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True, parents=True)
    
    if not DATASETS_AVAILABLE:
        print("Warning: datasets library not available. Falling back to URL download.")
        return download_sample_images_fallback(output_dir, max_images)
    
    print(f"Downloading sample images from HuggingFace to {output_dir}...")
    
    dataset_options = [
        {"name": "huggan/smithsonian_butterflies_subset", "image_column": "image", "split": "train"},
        {"name": "nlphuji/flickr30k", "image_column": "image", "split": "test"},
        {"name": "detection-datasets/coco", "image_column": "image", "split": "train"},
    ]
    
    if dataset_name:
        chosen_dataset = {"name": dataset_name, "image_column": "image", "split": "train"}
    else:
        chosen_dataset = None
        for ds_option in dataset_options:
            try:
                print(f"Trying dataset: {ds_option['name']}")
                # Test if dataset is accessible
                from datasets import load_dataset
                test_ds = load_dataset(ds_option["name"], split=f"{ds_option['split']}[:1]", trust_remote_code=True)
                chosen_dataset = ds_option
                print(f"Successfully found dataset: {ds_option['name']}")
                break
            except Exception as e:
                print(f"  Dataset {ds_option['name']} not accessible: {str(e)[:100]}...")
                continue
    
    if chosen_dataset is None:
        print("No accessible HuggingFace datasets found. Falling back to URL download.")
        return download_sample_images_fallback(output_dir, max_images)
    
    try:
        from datasets import load_dataset
        
        # Load a subset of the dataset
        max_to_load = min(max_images * 2, 100)  # Load extra in case some images fail
        dataset = load_dataset(
            chosen_dataset["name"], 
            split=f"{chosen_dataset['split']}[:{max_to_load}]",
            trust_remote_code=True
        )
        
        print(f"Loaded {len(dataset)} samples from {chosen_dataset['name']}")
        
        downloaded_files = []
        image_col = chosen_dataset["image_column"]
        
        for i, sample in enumerate(dataset):
            if len(downloaded_files) >= max_images:
                break
                
            try:
                if image_col not in sample or sample[image_col] is None:
                    continue
                
                image = sample[image_col]
                
                # Handle different image formats
                if hasattr(image, 'save'):  # PIL Image
                    pil_image = image
                elif isinstance(image, dict) and 'bytes' in image:
                    # Handle encoded image
                    import io
                    from PIL import Image
                    pil_image = Image.open(io.BytesIO(image['bytes']))
                else:
                    print(f"  Skipping unsupported image format for sample {i}")
                    continue
                
                # Convert to RGB and save
                if pil_image.mode != 'RGB':
                    pil_image = pil_image.convert('RGB')
                
                # Generate filename with more info
                caption = ""
                if 'caption' in sample and sample['caption']:
                    # Clean caption for filename
                    caption = re.sub(r'[^\w\s-]', '', str(sample['caption'])[:30])
                    caption = re.sub(r'[-\s]+', '_', caption)
                
                if caption:
                    image_path = output_path / f"hf_image_{i:03d}_{caption}.jpg"
                else:
                    image_path = output_path / f"hf_image_{i:03d}.jpg"
                
                pil_image.save(image_path, "JPEG", quality=95)
                downloaded_files.append(image_path)
                print(f"  Downloaded: {image_path.name}")
                
            except Exception as e:
                print(f"  Error processing sample {i}: {e}")
                continue
        
        if not downloaded_files:
            print("No images successfully downloaded from HuggingFace dataset. Falling back to URL download.")
            return download_sample_images_fallback(output_dir, max_images)
        
        print(f"Successfully downloaded {len(downloaded_files)} images from HuggingFace dataset '{chosen_dataset['name']}'")
        return downloaded_files
        
    except Exception as e:
        print(f"Error loading HuggingFace dataset: {e}")
        print("Falling back to URL download.")
        return download_sample_images_fallback(output_dir, max_images)


def download_sample_images_fallback(output_dir: str, max_images: int = 20) -> List[Path]:
    """Fallback function to download sample images from URLs."""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True, parents=True)
    
    print(f"Using fallback URL download to {output_dir}...")
    
        # Instead of external URLs, use a reliable HuggingFace dataset as fallback
    print(f"Using HuggingFace fallback dataset...")
    try:
        from datasets import load_dataset
        fallback_dataset = load_dataset('huggan/smithsonian_butterflies_subset', split=f'train[:{max_images*2}]', trust_remote_code=True)
        
        downloaded_files = []
        for i, sample in enumerate(fallback_dataset):
            if len(downloaded_files) >= max_images:
                break
                
            if 'image' in sample and sample['image'] is not None:
                try:
                    image = sample['image']
                    if hasattr(image, 'save'):
                        image_path = output_path / f"fallback_hf_image_{i:03d}.jpg"
                        if image.mode != 'RGB':
                            image = image.convert('RGB')
                        image.save(image_path, "JPEG", quality=95)
                        downloaded_files.append(image_path)
                        print(f"  Downloaded: {image_path.name}")
                except Exception as e:
                    print(f"  Error processing fallback image {i}: {e}")
                    continue
        
        return downloaded_files
        
    except Exception as e:
        print(f"HuggingFace fallback also failed: {e}")
        return []


def load_images_from_directory(image_dir: str, max_images: int = None, dataset_name: str = None) -> List[Path]:
    """Load images from a directory, with option to download if directory doesn't exist."""
    image_dir_path = Path(image_dir)
    
    if not image_dir_path.exists():
        print(f"Image directory {image_dir} not found.")
        download_count = max_images if max_images else 10
        downloaded_files = download_sample_images_from_hf(image_dir, download_count, dataset_name)
        if not downloaded_files:
            raise FileNotFoundError(f"Could not create or populate image directory: {image_dir}")
        return downloaded_files
    
    # Support common image formats
    image_extensions = ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.tiff", "*.webp"]
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(image_dir_path.glob(ext))
        image_files.extend(image_dir_path.glob(ext.upper()))
    
    image_files = sorted(image_files)
    
    if max_images:
        image_files = image_files[:max_images]
    
    if not image_files:
        print(f"No image files found in {image_dir}")
        download_count = max_images if max_images else 10
        downloaded_files = download_sample_images_from_hf(image_dir, download_count, dataset_name)
        if not downloaded_files:
            raise ValueError(f"Could not find or download images for {image_dir}")
        return downloaded_files
    
    print(f"Found {len(image_files)} images in {image_dir}")
    return image_files


def create_batches(items: List, batch_size: int) -> List[List]:
    """Split items into batches of specified size."""
    batches = []
    for i in range(0, len(items), batch_size):
        batches.append(items[i:i + batch_size])
    return batches


def load_model_and_processor(model_name: str, device: str = "auto"):
    """Load the multimodal model and processor with better error handling."""
    print(f"Loading model: {model_name}")
    
    # Determine device
    if device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
            print(f"Using CUDA device: {torch.cuda.get_device_name()}")
        else:
            device = "cpu"
            print("Using CPU device")
    
    # Model-specific loading logic
    model_name_lower = model_name.lower()
    
    try:
        if "qwen2-vl" in model_name_lower:
            print("Loading Qwen2-VL model...")
            processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            
        elif "llava-1.5" in model_name_lower or "llava-v1.5" in model_name_lower:
            print("Loading LLaVA 1.5 model...")
            processor = LlavaProcessor.from_pretrained(model_name)
            model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            
        elif "llava-next" in model_name_lower or "llava-v1.6" in model_name_lower:
            print("Loading LLaVA-Next model...")
            processor = LlavaNextProcessor.from_pretrained(model_name)
            model = LlavaNextForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            
        else:
            # Generic AutoProcessor approach
            print("Loading with AutoProcessor...")
            processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
            model = AutoModelForVision2Seq.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
        
        if device == "cpu":
            model = model.to(device)
            
        print(f"Successfully loaded {model_name}")
        return model, processor, device
        
    except Exception as e:
        print(f"Error loading model {model_name}: {e}")
        raise Exception(f"Could not load model {model_name}: {e}")


def prepare_inputs_for_model(processor, prompt: str, image, model_name: str, device: str):
    """Prepare inputs for different model types with proper formatting."""
    model_name_lower = model_name.lower()
    
    if "qwen2-vl" in model_name_lower:
        # Qwen2-VL format
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], images=[image], return_tensors="pt", padding=True)
        
    elif "llava" in model_name_lower:
        if "llava-1.5" in model_name_lower or "llava-v1.5" in model_name_lower:
            formatted_prompt = f"USER: <image>\n{prompt} ASSISTANT:"
        else:
            formatted_prompt = f"<|im_start|>user\n<image>\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = processor(text=formatted_prompt, images=image, return_tensors="pt", padding=True)
        
    else:
        inputs = processor(text=prompt, images=image, return_tensors="pt", padding=True)
    
    if device == "cuda":
        inputs = {k: v.to(device) for k, v in inputs.items()}
    
    return inputs


def run_multimodal_inference(
    model_name: str,
    image_files: List[Path],
    batch_size: int,
    prompt: str = None,
    max_input_tokens: int = 1024,
    max_output_tokens: int = 1024,
    temperature: float = 0.8,
    device: str = "auto"
) -> Dict[str, Any]:
    """
    Run multimodal inference on a set of images with specified batch size using Hugging Face Transformers.
    """
    
    if not TRANSFORMERS_AVAILABLE:
        return {"error": "Transformers not available. Please install transformers package."}
    
    if not PIL_AVAILABLE:
        return {"error": "PIL not available. Please install Pillow package."}
    
    if prompt is None:
        prompt = "What is shown in this image? Describe it in detail."
    
    print(f"Initializing model: {model_name}")
    print(f"Max input tokens: {max_input_tokens}")
    print(f"Max output tokens: {max_output_tokens}")
    
    model_init_start = time.perf_counter()
    
    try:
        model, processor, actual_device = load_model_and_processor(model_name, device)
    except Exception as e:
        print(f"Error initializing model {model_name}: {e}")
        return {"error": str(e)}
    
    model_init_time = time.perf_counter() - model_init_start
    print(f"Model initialization time: {model_init_time:.4f} seconds")
    
    # Create batches
    batches = create_batches(image_files, batch_size)
    
    # Track metrics
    metrics = {
        "model_name": model_name,
        "device": actual_device,
        "total_images": len(image_files),
        "batch_size": batch_size,
        "num_batches": len(batches),
        "max_input_tokens": max_input_tokens,
        "max_output_tokens": max_output_tokens,
        "model_init_time": model_init_time,
        "batch_times": [],
        "total_inference_time": 0,
        "average_time_per_image": 0,
        "images_per_second": 0,
        "total_input_tokens": 0,
        "total_output_tokens": 0,
        "average_input_tokens_per_response": 0,
        "average_output_tokens_per_response": 0,
        "system_info": get_system_info()
    }
    
    print(f"\nRunning inference on {len(image_files)} images in {len(batches)} batches of size {batch_size}")
    print("=" * 60)
    
    all_outputs = []
    total_inference_start = time.perf_counter()
    
    for batch_idx, batch_files in enumerate(batches):
        print(f"Processing batch {batch_idx + 1}/{len(batches)} ({len(batch_files)} images)")
        
        # Prepare batch
        batch_load_start = time.perf_counter()
        batch_images = []
        valid_files = []
        
        for img_path in batch_files:
            try:
                image = Image.open(img_path).convert("RGB")
                batch_images.append(image)
                valid_files.append(img_path)
            except Exception as e:
                print(f"Error loading image {img_path}: {e}")
                continue
        
        batch_load_time = time.perf_counter() - batch_load_start
        
        if not batch_images:
            print(f"No valid images in batch {batch_idx + 1}, skipping...")
            continue
        
        # Run batch inference
        batch_inference_start = time.perf_counter()
        
        try:
            batch_outputs = []
            batch_input_tokens = 0
            batch_output_tokens = 0
            
            # Process each image individually for now (avoids batching issues)
            for i, (image, img_path) in enumerate(zip(batch_images, valid_files)):
                try:
                    # Prepare inputs with proper formatting
                    inputs = prepare_inputs_for_model(processor, prompt, image, model_name, actual_device)
                    
                    # Count input tokens
                    input_token_count = inputs['input_ids'].shape[1]
                    
                    # Truncate input if too long
                    if input_token_count > max_input_tokens:
                        print(f"  Truncating input from {input_token_count} to {max_input_tokens} tokens")
                        inputs['input_ids'] = inputs['input_ids'][:, :max_input_tokens]
                        if 'attention_mask' in inputs:
                            inputs['attention_mask'] = inputs['attention_mask'][:, :max_input_tokens]
                        input_token_count = max_input_tokens
                    
                    with torch.no_grad():
                        generated_ids = model.generate(
                            **inputs,
                            max_new_tokens=max_output_tokens,
                            min_new_tokens=1,
                            temperature=temperature if temperature > 0 else None,
                            do_sample=True if temperature > 0 else False,
                            pad_token_id=processor.tokenizer.eos_token_id if hasattr(processor, 'tokenizer') else None,
                            use_cache=True
                        )
                    
                    # Calculate output tokens (excluding input)
                    output_token_count = generated_ids.shape[1] - input_token_count
                    
                    # Decode the response
                    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    
                    # Clean up the response (remove input prompt)
                    if "ASSISTANT:" in generated_text:
                        generated_text = generated_text.split("ASSISTANT:")[-1].strip()
                    elif "<|im_start|>assistant" in generated_text:
                        generated_text = generated_text.split("<|im_start|>assistant")[-1].strip()
                    elif prompt in generated_text:
                        generated_text = generated_text.replace(prompt, "").strip()
                    
                    batch_outputs.append(generated_text)
                    batch_input_tokens += input_token_count
                    batch_output_tokens += output_token_count
                    
                    print(f"    Image {i+1}: {input_token_count} input tokens, {output_token_count} output tokens")
                    
                except Exception as e:
                    print(f"    Error processing image {img_path}: {e}")
                    batch_outputs.append(f"<ERROR: {str(e)}>")
                    continue
            
            batch_inference_time = time.perf_counter() - batch_inference_start
            
            # Process outputs
            for i, (img_path, output_text) in enumerate(zip(valid_files, batch_outputs)):
                all_outputs.append({
                    "image_path": str(img_path),
                    "generated_text": output_text,
                    "batch_idx": batch_idx,
                    "input_tokens": batch_input_tokens // len(batch_images) if batch_images else 0,
                    "output_tokens": batch_output_tokens // len(batch_images) if batch_images else 0
                })
            
            metrics["batch_times"].append({
                "batch_idx": batch_idx,
                "load_time": batch_load_time,
                "inference_time": batch_inference_time,
                "total_time": batch_load_time + batch_inference_time,
                "images_in_batch": len(batch_images),
                "input_tokens": batch_input_tokens,
                "output_tokens": batch_output_tokens
            })
            
            metrics["total_input_tokens"] += batch_input_tokens
            metrics["total_output_tokens"] += batch_output_tokens
            
            print(f"  Batch {batch_idx + 1} completed in {batch_inference_time:.4f}s "
                  f"(+{batch_load_time:.4f}s loading)")
            print(f"    Total tokens: {batch_input_tokens} input, {batch_output_tokens} output")
            
        except Exception as e:
            print(f"Error during batch {batch_idx + 1} inference: {e}")
            metrics["batch_times"].append({
                "batch_idx": batch_idx,
                "error": str(e)
            })
    
    total_inference_time = time.perf_counter() - total_inference_start
    
    # Calculate final metrics
    metrics["total_inference_time"] = total_inference_time
    metrics["average_time_per_image"] = total_inference_time / len(image_files) if len(image_files) > 0 else 0
    metrics["images_per_second"] = len(image_files) / total_inference_time if total_inference_time > 0 else 0
    metrics["average_input_tokens_per_response"] = metrics["total_input_tokens"] / len(image_files) if len(image_files) > 0 else 0
    metrics["average_output_tokens_per_response"] = metrics["total_output_tokens"] / len(image_files) if len(image_files) > 0 else 0
    
    # Calculate batch statistics
    valid_batch_times = [b["inference_time"] for b in metrics["batch_times"] if "inference_time" in b]
    if valid_batch_times:
        metrics["batch_stats"] = {
            "min_batch_time": min(valid_batch_times),
            "max_batch_time": max(valid_batch_times),
            "avg_batch_time": statistics.mean(valid_batch_times),
            "median_batch_time": statistics.median(valid_batch_times),
            "std_batch_time": statistics.stdev(valid_batch_times) if len(valid_batch_times) > 1 else 0
        }
    
    metrics["outputs"] = all_outputs
    
    return metrics


def print_metrics(metrics: Dict[str, Any]):
    """Print performance metrics in a formatted way."""
    print("\n" + "=" * 60)
    print("PERFORMANCE METRICS")
    print("=" * 60)
    
    if "error" in metrics:
        print(f"ERROR: {metrics['error']}")
        return
    
    print(f"Model: {metrics['model_name']}")
    print(f"Device: {metrics['device']}")
    print(f"Total Images: {metrics['total_images']}")
    print(f"Batch Size: {metrics['batch_size']}")
    print(f"Number of Batches: {metrics['num_batches']}")
    print(f"Max Input/Output Tokens: {metrics['max_input_tokens']}/{metrics['max_output_tokens']}")
    print()
    
    print("TIMING METRICS:")
    print(f"  Model Initialization: {metrics['model_init_time']:.4f} seconds")
    print(f"  Total Inference Time: {metrics['total_inference_time']:.4f} seconds")
    print(f"  Average Time per Image: {metrics['average_time_per_image']:.4f} seconds")
    print(f"  Images per Second: {metrics['images_per_second']:.2f}")
    print()
    
    print("TOKEN METRICS:")
    print(f"  Total Input Tokens: {metrics['total_input_tokens']}")
    print(f"  Total Output Tokens: {metrics['total_output_tokens']}")
    print(f"  Average Input Tokens per Response: {metrics['average_input_tokens_per_response']:.2f}")
    print(f"  Average Output Tokens per Response: {metrics['average_output_tokens_per_response']:.2f}")
    print()
    
    if "batch_stats" in metrics:
        batch_stats = metrics["batch_stats"]
        print("BATCH STATISTICS:")
        print(f"  Min Batch Time: {batch_stats['min_batch_time']:.4f} seconds")
        print(f"  Max Batch Time: {batch_stats['max_batch_time']:.4f} seconds")
        print(f"  Average Batch Time: {batch_stats['avg_batch_time']:.4f} seconds")
        print(f"  Median Batch Time: {batch_stats['median_batch_time']:.4f} seconds")
        print(f"  Std Dev Batch Time: {batch_stats['std_batch_time']:.4f} seconds")
        print()
    
    print("SYSTEM INFO:")
    sys_info = metrics["system_info"]
    print(f"  CPU Cores: {sys_info['cpu_count']}")
    print(f"  Total Memory: {sys_info['memory_total_gb']}")
    if sys_info["gpus"]:
        for i, gpu in enumerate(sys_info["gpus"]):
            print(f"  GPU {i}: {gpu['name']}")
            if 'memory_total' in gpu:
                print(f"    Memory: {gpu['memory_total']} MB")


def main():
    parser = argparse.ArgumentParser(
        description="Multimodal Model Inference Benchmark using Hugging Face Transformers (Fixed Version)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--model", 
        type=str, 
        default="llava-hf/llava-1.5-7b-hf",
        help="Model name or path (HuggingFace transformers compatible)"
    )
    
    parser.add_argument(
        "--batch-size", 
        type=int, 
        default=2,
        help="Batch size for inference"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="Device to use for inference"
    )
    
    parser.add_argument(
        "--image-dir", 
        type=str, 
        default="sample_images",
        help="Directory containing images"
    )
    
    parser.add_argument(
        "--max-images", 
        type=int, 
        default=100,
        help="Maximum number of images to process"
    )
    

    parser.add_argument(
        "--dataset-name", 
        type=str, 
        default=None,
        help="Specific HuggingFace dataset name to use for downloading images"
    )
    parser.add_argument(
        "--prompt", 
        type=str, 
        default=None,
        help="Custom prompt for the model"
    )
    
    parser.add_argument(
        "--max-input-tokens", 
        type=int, 
        default=1024,
        help="Maximum input tokens"
    )
    
    parser.add_argument(
        "--max-output-tokens", 
        type=int, 
        default=1024,
        help="Maximum output tokens to generate"
    )
    
    parser.add_argument(
        "--temperature", 
        type=float, 
        default=0.7,
        help="Sampling temperature"
    )
    
    parser.add_argument(
        "--output-file", 
        type=str, 
        default=None,
        help="Output file to save results (JSON format)"
    )
    
    parser.add_argument(
        "--show-outputs", 
        action="store_true",
        help="Show generated text outputs"
    )
    
    args = parser.parse_args()
    
    print("Multimodal Model Inference Benchmark (Hugging Face Transformers - Fixed)")
    print("=" * 75)
    print(f"Model: {args.model}")
    print(f"Device: {args.device}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Image Directory: {args.image_dir}")
    print(f"Max Images: {args.max_images}")
    print(f"Token Limits: {args.max_input_tokens} input, {args.max_output_tokens} output")
    print()
    
    # Check dependencies
    missing_deps = []
    if not TRANSFORMERS_AVAILABLE:
        missing_deps.append("transformers torch")
    if not PIL_AVAILABLE:
        missing_deps.append("Pillow")
    
    if missing_deps:
        print("ERROR: Missing required dependencies:")
        for dep in missing_deps:
            print(f"  - {dep}")
        print("\nPlease install missing dependencies:")
        print(f"pip install {' '.join(missing_deps)}")
        return 1
    
    try:
        # Load images from directory (will download if needed)
        image_files = load_images_from_directory(args.image_dir, args.max_images, getattr(args, "dataset_name", None))
        
        # Run inference
        metrics = run_multimodal_inference(
            model_name=args.model,
            image_files=image_files,
            batch_size=args.batch_size,
            prompt=args.prompt,
            max_input_tokens=args.max_input_tokens,
            max_output_tokens=args.max_output_tokens,
            temperature=args.temperature,
            device=args.device
        )
        
        # Print metrics
        print_metrics(metrics)
        
        # Show outputs if requested
        if args.show_outputs and "outputs" in metrics:
            print("\n" + "=" * 60)
            print("GENERATED OUTPUTS")
            print("=" * 60)
            for output in metrics["outputs"]:
                print(f"\nImage: {Path(output['image_path']).name}")
                print(f"Input Tokens: {output.get('input_tokens', 'N/A')}")
                print(f"Output Tokens: {output.get('output_tokens', 'N/A')}")
                print(f"Generated: {output['generated_text']}")
                print("-" * 40)
        
        # Save results if requested
        if args.output_file:
            with open(args.output_file, 'w') as f:
                json.dump(metrics, f, indent=2, default=str)
            print(f"\nResults saved to: {args.output_file}")
            
    except KeyboardInterrupt:
        print("\nBenchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
